"""
Bias Detection Service
======================

Detecta y mitiga sesgos en contenido generado por IA.

Tipos de sesgos detectados:
- GÃ©nero (referencias desbalanceadas a gÃ©neros)
- Edad (suposiciones basadas en edad/antigÃ¼edad)
- Origen/Nacionalidad (referencias Ã©tnicas o geogrÃ¡ficas)
- Discapacidad (lenguaje discriminatorio)
- Estereotipos profesionales (roles asociados a gÃ©nero/edad)

Estrategias:
1. Pre-generaciÃ³n: Guardrails en prompts
2. Post-generaciÃ³n: AnÃ¡lisis de contenido generado
3. ValidaciÃ³n: Checks automÃ¡ticos y manual review triggers
"""

import re
from typing import List, Dict, Tuple, Optional
from dataclasses import dataclass


@dataclass
class BiasPattern:
    """PatrÃ³n de sesgo a detectar."""
    category: str
    pattern: str  # Regex pattern
    severity: str  # 'high', 'medium', 'low'
    description: str
    mitigation: str


class BiasDetector:
    """
    Detector de sesgos en contenido generado por IA.
    """
    
    def __init__(self):
        self.bias_patterns = self._initialize_bias_patterns()
        self.neutral_language_guide = self._initialize_neutral_language()
    
    def _initialize_bias_patterns(self) -> List[BiasPattern]:
        """Inicializa patrones de detecciÃ³n de sesgos."""
        patterns = []
        
        # GÃ‰NERO
        patterns.extend([
            BiasPattern(
                category='gender',
                pattern=r'\b(Ã©l|ella|masculino|femenino)\b(?!.*\b(independientemente|sin importar|cualquier)\b)',
                severity='high',
                description='Referencias explÃ­citas a gÃ©nero sin justificaciÃ³n',
                mitigation='Usar lenguaje neutral o inclusivo'
            ),
            BiasPattern(
                category='gender',
                pattern=r'\b(hombres|mujeres)\s+(son|tienden|suelen)\b',
                severity='high',
                description='GeneralizaciÃ³n por gÃ©nero',
                mitigation='Evitar generalizaciones basadas en gÃ©nero'
            ),
            BiasPattern(
                category='gender',
                pattern=r'\b(lÃ­der|ingeniero|secretaria|enfermera)\b.*\b(Ã©l|ella)\b',
                severity='medium',
                description='AsociaciÃ³n de roles con gÃ©nero especÃ­fico',
                mitigation='Usar lenguaje neutro para roles profesionales'
            )
        ])
        
        # EDAD
        patterns.extend([
            BiasPattern(
                category='age',
                pattern=r'\b(joven|viejo|mayor|antiguo)\s+(empleado|profesional|trabajador)\b',
                severity='high',
                description='Referencias a edad de forma innecesaria',
                mitigation='Evitar mencionar edad salvo sea estrictamente relevante'
            ),
            BiasPattern(
                category='age',
                pattern=r'\b(millennials?|generaciÃ³n [XYZ]|boomers?)\b',
                severity='medium',
                description='Estereotipos generacionales',
                mitigation='Evitar categorizar por generaciÃ³n'
            ),
            BiasPattern(
                category='age',
                pattern=r'\b(demasiado (joven|viejo)|muy (senior|junior))\b',
                severity='high',
                description='Juicios basados en edad/antigÃ¼edad',
                mitigation='Basar evaluaciones en competencias, no edad'
            )
        ])
        
        # ORIGEN/NACIONALIDAD
        patterns.extend([
            BiasPattern(
                category='origin',
                pattern=r'\b(espaÃ±ol|extranjero|latino|asiÃ¡tico|africano|europeo)\s+(empleado|trabajador)\b',
                severity='high',
                description='Referencias a origen Ã©tnico/nacional',
                mitigation='No mencionar origen salvo sea relevante para visas/permisos'
            ),
            BiasPattern(
                category='origin',
                pattern=r'\b(acento|cultura|tradiciÃ³n)\s+(de su paÃ­s|nativa|extranjera)\b',
                severity='medium',
                description='Referencias a diferencias culturales',
                mitigation='Evitar mencionar diferencias culturales'
            )
        ])
        
        # DISCAPACIDAD
        patterns.extend([
            BiasPattern(
                category='disability',
                pattern=r'\b(limitaciÃ³n|impedimento|deficiencia|incapacidad)\b',
                severity='high',
                description='Lenguaje discriminatorio sobre capacidades',
                mitigation='Usar lenguaje inclusivo y respetuoso'
            ),
            BiasPattern(
                category='disability',
                pattern=r'\b(normal|anormal|sufre de|padece)\b',
                severity='high',
                description='Lenguaje que implica condiciones como defectos',
                mitigation='Usar lenguaje neutral y objetivo'
            )
        ])
        
        # ESTEREOTIPOS PROFESIONALES
        patterns.extend([
            BiasPattern(
                category='stereotype',
                pattern=r'\b(como es tÃ­pico|como suele pasar|la mayorÃ­a de)\s+los?\s+(hombres|mujeres|jÃ³venes|mayores)\b',
                severity='high',
                description='Estereotipos profesionales',
                mitigation='Basar recomendaciones en datos individuales, no estereotipos'
            ),
            BiasPattern(
                category='stereotype',
                pattern=r'\b(apto para|ideal para|mejor suited para)\s+(hombres|mujeres|jÃ³venes)\b',
                severity='high',
                description='Roles asociados a grupos demogrÃ¡ficos',
                mitigation='No asociar roles con caracterÃ­sticas demogrÃ¡ficas'
            )
        ])
        
        # LENGUAJE NO INCLUSIVO
        patterns.extend([
            BiasPattern(
                category='language',
                pattern=r'\b(los empleados|los profesionales|los trabajadores)\b(?!.*\b(y empleadas|y profesionales|y trabajadoras)\b)',
                severity='low',
                description='Lenguaje no inclusivo (masculino genÃ©rico)',
                mitigation='Usar lenguaje inclusivo: "el personal", "las personas"'
            ),
            BiasPattern(
                category='language',
                pattern=r'\b(padre de familia|ama de casa|cabeza de familia)\b',
                severity='medium',
                description='TÃ©rminos con connotaciones de gÃ©nero',
                mitigation='Usar tÃ©rminos neutrales'
            )
        ])
        
        return patterns
    
    def _initialize_neutral_language(self) -> Dict[str, str]:
        """GuÃ­a de lenguaje neutral."""
        return {
            'los empleados': 'el personal / las personas empleadas',
            'los trabajadores': 'el personal / el equipo',
            'los profesionales': 'las personas profesionales / el equipo',
            'Ã©l/ella': 'la persona / el empleado/a',
            'lÃ­der masculino': 'lÃ­der',
            'ingeniero/a': 'profesional de ingenierÃ­a',
            'joven empleado': 'empleado en etapa temprana / empleado junior',
            'empleado senior': 'empleado con experiencia / empleado de nivel senior',
            'antigÃ¼edad': 'experiencia / trayectoria',
        }
    
    def detect_bias(self, text: str) -> Dict[str, any]:
        """
        Detecta sesgos en un texto.
        
        Args:
            text: Texto a analizar
            
        Returns:
            Diccionario con resultados de detecciÃ³n
        """
        detections = []
        text_lower = text.lower()
        
        for pattern in self.bias_patterns:
            matches = re.finditer(pattern.pattern, text_lower, re.IGNORECASE)
            for match in matches:
                detections.append({
                    'category': pattern.category,
                    'severity': pattern.severity,
                    'matched_text': match.group(0),
                    'position': match.span(),
                    'description': pattern.description,
                    'mitigation': pattern.mitigation
                })
        
        # Calcular score de sesgo
        has_bias = len(detections) > 0
        high_severity_count = len([d for d in detections if d['severity'] == 'high'])
        
        # Confidence basado en nÃºmero y severidad de detecciones
        if high_severity_count >= 2:
            confidence = 0.95
        elif high_severity_count == 1:
            confidence = 0.80
        elif len(detections) >= 3:
            confidence = 0.70
        elif len(detections) > 0:
            confidence = 0.60
        else:
            confidence = 0.95  # Alta confianza en ausencia de sesgo
        
        # CategorÃ­as Ãºnicas detectadas
        categories = list(set([d['category'] for d in detections]))
        
        # Recomendaciones
        recommendations = self._generate_bias_mitigation_recommendations(detections)
        
        return {
            'has_bias': has_bias,
            'bias_score': min(len(detections) / 10.0, 1.0),  # 0-1
            'confidence': confidence,
            'total_detections': len(detections),
            'high_severity_count': high_severity_count,
            'bias_types_detected': categories,
            'flagged_content': detections,
            'recommendations': recommendations,
            'requires_human_review': high_severity_count > 0
        }
    
    def _generate_bias_mitigation_recommendations(self, detections: List[Dict]) -> List[str]:
        """Genera recomendaciones especÃ­ficas para mitigar sesgos detectados."""
        recommendations = []
        
        categories_found = set([d['category'] for d in detections])
        
        if 'gender' in categories_found:
            recommendations.append('Usar lenguaje inclusivo y neutro en gÃ©nero')
            recommendations.append('Basar recomendaciones en competencias, no en gÃ©nero')
        
        if 'age' in categories_found:
            recommendations.append('Evitar referencias a edad o antigÃ¼edad innecesarias')
            recommendations.append('Enfocarse en skills y experiencia relevante, no edad')
        
        if 'origin' in categories_found:
            recommendations.append('No mencionar origen Ã©tnico o nacional salvo sea legalmente relevante')
        
        if 'disability' in categories_found:
            recommendations.append('Usar lenguaje respetuoso e inclusivo respecto a capacidades')
        
        if 'stereotype' in categories_found:
            recommendations.append('Evitar estereotipos profesionales basados en demografÃ­a')
            recommendations.append('Personalizar recomendaciones basÃ¡ndose en datos individuales')
        
        if 'language' in categories_found:
            recommendations.append('Adoptar lenguaje inclusivo consistente')
        
        # RecomendaciÃ³n general
        if recommendations:
            recommendations.insert(0, 'GENERAL: Revisar todo el contenido para eliminar sesgos identificados')
        
        return recommendations
    
    def sanitize_text(self, text: str, strict_mode: bool = False) -> Tuple[str, List[str]]:
        """
        Intenta sanitizar un texto reemplazando tÃ©rminos sesgados.
        
        Args:
            text: Texto a sanitizar
            strict_mode: Si True, reemplaza todo. Si False, solo tÃ©rminos crÃ­ticos.
            
        Returns:
            (texto_sanitizado, cambios_realizados)
        """
        sanitized = text
        changes = []
        
        # Aplicar reemplazos de lenguaje neutral
        for biased, neutral in self.neutral_language_guide.items():
            if biased in sanitized.lower():
                # Reemplazar manteniendo capitalizaciÃ³n
                pattern = re.compile(re.escape(biased), re.IGNORECASE)
                matches = pattern.findall(sanitized)
                for match in matches:
                    sanitized = sanitized.replace(match, neutral)
                    changes.append(f'Reemplazado "{match}" por "{neutral}"')
        
        return sanitized, changes
    
    def validate_prompt(self, prompt: str) -> Dict[str, any]:
        """
        Valida un prompt antes de enviarlo a la IA para prevenir sesgos.
        
        Args:
            prompt: Prompt a validar
            
        Returns:
            Resultado de validaciÃ³n con warnings
        """
        result = self.detect_bias(prompt)
        
        warnings = []
        if result['has_bias']:
            warnings.append('âš ï¸ El prompt contiene potenciales sesgos que pueden propagarse a la respuesta')
            warnings.extend(result['recommendations'])
        
        # Checks adicionales en prompts
        if 'mejor candidato' in prompt.lower() and not any(
            word in prompt.lower() for word in ['competencias', 'skills', 'experiencia']
        ):
            warnings.append('âš ï¸ Asegurar que criterios de evaluaciÃ³n sean objetivos y basados en competencias')
        
        return {
            'is_valid': not result['requires_human_review'],
            'bias_detected': result['has_bias'],
            'warnings': warnings,
            'full_analysis': result
        }
    
    def create_bias_free_prompt_template(self, context: str = 'general') -> str:
        """
        Crea template de prompt con guardrails contra sesgos.
        
        Args:
            context: Contexto del anÃ¡lisis ('recommendations', 'narrative', 'general')
            
        Returns:
            Template de prompt con guardrails incorporados
        """
        base_guardrails = """
INSTRUCCIONES CRÃTICAS - NEUTRALIDAD Y EQUIDAD:

1. LENGUAJE INCLUSIVO: Usar siempre lenguaje neutral en gÃ©nero (ej: "el personal", "las personas")
2. NO ASUMIR GÃ‰NERO: No hacer suposiciones sobre gÃ©nero de personas
3. NO MENCIONAR EDAD: No referenciar edad o antigÃ¼edad de forma innecesaria
4. NO MENCIONAR ORIGEN: No hacer referencias a nacionalidad, origen Ã©tnico, o cultural
5. BASAR EN DATOS: Todas las recomendaciones deben basarse EXCLUSIVAMENTE en:
   - Competencias tÃ©cnicas (skills)
   - Experiencia profesional relevante
   - Responsabilidades actuales
   - Ambiciones profesionales declaradas
   - Performance objetiva

6. EVITAR ESTEREOTIPOS: No usar estereotipos profesionales asociados a grupos demogrÃ¡ficos
7. SER OBJETIVO: Usar mÃ©tricas y datos cuantitativos cuando sea posible

Si no hay datos suficientes para una recomendaciÃ³n objetiva, indicarlo explÃ­citamente.
"""
        
        if context == 'recommendations':
            specific = """
CONTEXTO: Recomendaciones de desarrollo profesional

FOCO: Personalizar recomendaciones basÃ¡ndose en:
- Gap especÃ­fico de skills identificado
- Ambiciones profesionales del empleado
- Trayectoria de carrera individual
- Oportunidades de desarrollo disponibles

NO incluir suposiciones sobre capacidades basadas en caracterÃ­sticas personales.
"""
        elif context == 'narrative':
            specific = """
CONTEXTO: Narrativa ejecutiva

FOCO: AnÃ¡lisis agregado y trends organizacionales
- Usar estadÃ­sticas y mÃ©tricas agregadas
- Identificar patterns en datos, no en personas
- Hacer recomendaciones estratÃ©gicas objetivas
"""
        else:
            specific = ""
        
        return base_guardrails + specific
    
    def get_bias_report(self, detections: List[Dict]) -> str:
        """Genera reporte legible de sesgos detectados."""
        if not detections:
            return "âœ… No se detectaron sesgos en el contenido analizado."
        
        report = "âš ï¸ REPORTE DE SESGOS DETECTADOS\n"
        report += "=" * 50 + "\n\n"
        
        by_category = {}
        for d in detections:
            cat = d['category']
            if cat not in by_category:
                by_category[cat] = []
            by_category[cat].append(d)
        
        for category, items in by_category.items():
            report += f"\nðŸ“Š CategorÃ­a: {category.upper()}\n"
            report += f"   Detecciones: {len(items)}\n"
            for item in items:
                report += f"   - [{item['severity']}] {item['matched_text']}\n"
                report += f"     RazÃ³n: {item['description']}\n"
                report += f"     MitigaciÃ³n: {item['mitigation']}\n"
        
        return report
    
    def batch_validate(self, texts: List[str]) -> Dict[str, any]:
        """
        Valida mÃºltiples textos en batch.
        
        Args:
            texts: Lista de textos a validar
            
        Returns:
            Reporte agregado de validaciÃ³n
        """
        results = [self.detect_bias(text) for text in texts]
        
        total_texts = len(texts)
        texts_with_bias = sum(1 for r in results if r['has_bias'])
        total_detections = sum(r['total_detections'] for r in results)
        
        all_categories = set()
        for r in results:
            all_categories.update(r['bias_types_detected'])
        
        return {
            'total_texts_analyzed': total_texts,
            'texts_with_bias': texts_with_bias,
            'bias_rate': texts_with_bias / total_texts if total_texts > 0 else 0,
            'total_detections': total_detections,
            'categories_found': list(all_categories),
            'requires_review': any(r['requires_human_review'] for r in results),
            'individual_results': results
        }
